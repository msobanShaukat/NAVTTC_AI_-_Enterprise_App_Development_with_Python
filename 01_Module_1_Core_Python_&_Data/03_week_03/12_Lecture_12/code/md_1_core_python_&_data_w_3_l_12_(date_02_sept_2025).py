# -*- coding: utf-8 -*-
"""MD_1: Core_python_&_data_W#3_L#12_(Date:02-Sept-2025)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZuMjIY7q-yGg0kdc_k9OxTn4tDbg4ovL

# ğŸ’¾ Module 1: Core Python & Data - Week 3 Lecture 12
**Date:** 02/09/2025  
**Instructor:** Hamza Sajid  

## ğŸ—ƒï¸ Beyond Volatile Memory: Introduction to File I/O

**Welcome back!** Until now, our programs have had amnesia. Every time we run them, they start from a blank slate, and any data we createâ€”like a to-do list or a contact bookâ€”vanishes the moment the program ends. Today, we solve that. We'll learn how to **read from and write to files**, allowing our applications to save their state, load data, and interact with the outside world.

## ğŸ“‹ Today's Agenda

1.  **ğŸ“„ The Fundamentals of Text Files (.txt)**
    *   What is File I/O? Making Programs Remember
    *   The `with open(...)` Statement: The Safe and Modern Way
    *   File Modes: Reading (r), Writing (w), and Appending (a)
    *   Reading from Files: `.read()`, `.readlines()`, and Looping
    *   Writing to Files: `.write()`
    *   Interactive Exercise: Saving and Reading a To-Do List

2.  **ğŸ“Š Working with Structured Data (.csv & JSON)**
    *   The Problem with Plain Text for Complex Data
    *   Introduction to CSV (Comma-Separated Values)
    *   Reading CSV Files with the `csv` Module
    *   Introduction to JSON: The Language of Web APIs
    *   Using the `json` Module: `json.dump()` and `json.load()`
    *   Interactive Exercise: Saving and Loading a Dictionary as JSON

3.  **ğŸ† Best Practices & The Hands-On Lab**
    *   Error Handling for Files: `try...except FileNotFoundError`
    *   **Hands-On Lab:** Log File Analyzer
    *   Q&A and Wrap-up

## 1. ğŸ“„ The Fundamentals of Text Files (.txt)

### ğŸ¤” What is File I/O?

**File I/O (Input/Output)** is the process of a program reading data from a file (input) or writing data to a file (output). This is how we achieve **persistence**â€”the ability for data to survive after the program has stopped running.

Think of it like this:
- **RAM (Memory):** Temporary workspace (forgets everything when power is off)
- **Files (Storage):** Permanent storage (remembers even after power is off)
"""

# Let's see the difference between memory and file storage

# Data in memory (volatile - disappears when program ends)
tasks_in_memory = ["Buy groceries", "Pay bills", "Walk the dog"]
print("Tasks in memory:", tasks_in_memory)

# When program ends, this list is gone forever!
# Files let us save data permanently

"""### ğŸ›¡ï¸ The `with open(...)` Statement: The Safe Way

To work with a file, you must first open it. The modern, recommended way is with a `with` statement. This creates a context where the file is open, and it **automatically and safely closes the file for you** when you're done, even if errors occur.
"""

# The safe way to handle files
with open("example.txt", "w") as f:  # 'f' is the file object
    f.write("Hello, File World!\n")
    f.write("This is line 2.\n")

# File is automatically closed here, even if errors occurred above

# Let's read it back
with open("example.txt", "r") as f:
    content = f.read()
    print("File content:")
    print(content)

"""### ğŸ”§ File Modes: 'r', 'w', and 'a'

The second argument to `open()` is the **mode**. It tells Python what you intend to do with the file.

| Mode | Description | Creates File | Overwrites |
|:-----|:------------|:------------:|:----------:|
| **'r'** (Read) | Opens for reading | âŒ | âŒ |
| **'w'** (Write) | Opens for writing | âœ… | âœ… |
| **'a'** (Append) | Opens for appending | âœ… | âŒ |
"""

# Demonstration of different file modes

# 1. Write mode ('w') - creates new file or overwrites existing
with open("demo.txt", "w") as f:
    f.write("This is the original content.\n")

print("After writing:")
with open("demo.txt", "r") as f:
    print(f.read())

# 2. Append mode ('a') - adds to existing content
with open("demo.txt", "a") as f:
    f.write("This is appended content.\n")

print("After appending:")
with open("demo.txt", "r") as f:
    print(f.read())

# 3. Write mode again - overwrites everything!
with open("demo.txt", "w") as f:
    f.write("This completely replaces everything.\n")

print("After writing again:")
with open("demo.txt", "r") as f:
    print(f.read())

"""### ğŸ“– Reading from Files

There are several ways to read file content:

1. **`.read()`** - Reads entire file as a single string
2. **`.readlines()`** - Reads all lines into a list
3. **Looping** - Most memory-efficient for large files
"""

# Create a sample file first
with open("story.txt", "w") as f:
    f.write("This is line 1.\n")
    f.write("This is line 2.\n")
    f.write("This is line 3.\n")

# Method 1: read() - entire file as string
with open("story.txt", "r") as f:
    full_content = f.read()
    print("Using read():")
    print(repr(full_content))  # repr shows special characters
    print()

# Method 2: readlines() - list of lines
with open("story.txt", "r") as f:
    lines = f.readlines()
    print("Using readlines():")
    for i, line in enumerate(lines):
        print(f"Line {i+1}: {repr(line)}")
    print()

# Method 3: Looping - most efficient for large files
print("Using loop (most efficient):")
with open("story.txt", "r") as f:
    for line_number, line in enumerate(f, 1):
        print(f"Line {line_number}: {line.strip()}")  # strip() removes \n

"""### ğŸ“ Writing to Files

There are two main writing methods:

1. **`.write()`** - Writes a single string
2. **`.writelines()`** - Writes a list of strings
"""

# Writing examples
lines_to_write = ["First line.\n", "Second line.\n", "Third line.\n"]

# Using write()
with open("write_demo.txt", "w") as f:
    f.write("This is the first sentence.\n")
    f.write("This is the second sentence.\n")

# Using writelines()
with open("writelines_demo.txt", "w") as f:
    f.writelines(lines_to_write)

# Read and show both files
print("write() demo:")
with open("write_demo.txt", "r") as f:
    print(f.read())

print("writelines() demo:")
with open("writelines_demo.txt", "r") as f:
    print(f.read())

"""### ğŸ§  In-Class Exercise: Saving a To-Do List (Page 8)

**Instructions:**
1. Create a list of strings: `tasks = ["Buy groceries", "Pay bills", "Walk the dog"]`
2. Write this list to a file named `todo.txt`. Each task should be on a new line
3. Write a second piece of code that opens `todo.txt` for reading and prints each task
"""

# Exercise Solution: To-Do List Manager

# Step 1: Create and write to-do list
tasks = ["Buy groceries", "Pay bills", "Walk the dog"]

with open("todo.txt", "w") as f:
    for task in tasks:
        f.write(task + "\n")  # Add newline after each task

print("âœ… To-do list saved to todo.txt")

# Step 2: Read and display to-do list
print("\nğŸ“‹ Your To-Do List:")
print("=" * 20)

try:
    with open("todo.txt", "r") as f:
        for line_number, task in enumerate(f, 1):
            print(f"{line_number}. {task.strip()}")
except FileNotFoundError:
    print("No to-do list found. Create one first!")

"""## 2. ğŸ“Š Working with Structured Data (.csv & JSON)

### ğŸ¤” The Problem with Plain Text

Our `.txt` files are simple, but what if we wanted to save structured data like our contact book?

```python
contact_book = {
    "Alice": {"phone": "555-1234", "email": "alice@example.com"},
    "Bob": {"phone": "555-5678", "email": "bob@example.com"}
}

### ğŸ“Š Introduction to CSV

**CSV (Comma-Separated Values)** is a simple format for tabular data. Each line is a row, and commas separate the values in that row.

Example:

name,email,phone

Alice,alice@example.com,555-1234

Bob,bob@example.com,555-5678
"""

# First, let's create a CSV file manually
csv_content = """name,email,phone
Alice,alice@example.com,555-1234
Bob,bob@example.com,555-5678
Charlie,charlie@example.com,555-9012"""

with open("contacts.csv", "w") as f:
    f.write(csv_content)

print("Created contacts.csv")

"""### ğŸ Reading CSV Files with the `csv` Module

Python has a built-in `csv` module to make working with CSV files easy.
"""

import csv

# Reading CSV files
print("Reading CSV with csv module:")
print("=" * 30)

with open("contacts.csv", "r") as f:
    csv_reader = csv.reader(f)

    # Get header row
    headers = next(csv_reader)
    print(f"Headers: {headers}")

    # Read data rows
    for row_number, row in enumerate(csv_reader, 1):
        print(f"Row {row_number}: {row}")
        print(f"  Name: {row[0]}, Email: {row[1]}, Phone: {row[2]}")

# Writing CSV files
import csv

new_contacts = [
    ["David", "david@example.com", "555-3456"],
    ["Eva", "eva@example.com", "555-7890"]
]

with open("new_contacts.csv", "w", newline='') as f:
    csv_writer = csv.writer(f)
    csv_writer.writerow(["name", "email", "phone"])  # Write header
    csv_writer.writerows(new_contacts)  # Write all rows

print("Created new_contacts.csv")

"""### ğŸ† A Better Way: JSON

While CSV is good for tables, the best way to save native Python data structures like lists and dictionaries is **JSON (JavaScript Object Notation)**. It's human-readable and maps almost perfectly to Python's syntax.

The `json` module is the tool for this:
- `json.dump(python_object, file_object)`: **Dumps** a Python object into a file
- `json.load(file_object)`: **Loads** a JSON file back into a Python object
"""

import json

# Our complex data structure
contact_book = {
    "Alice": {
        "phone": "555-1234",
        "email": "alice@example.com",
        "active": True
    },
    "Bob": {
        "phone": "555-5678",
        "email": "bob@example.com",
        "active": False
    }
}

# Saving to JSON file
with open("contacts.json", "w") as f:
    json.dump(contact_book, f, indent=4)  # indent makes it readable

print("âœ… Saved contacts to contacts.json")

# Reading from JSON file
with open("contacts.json", "r") as f:
    loaded_contacts = json.load(f)

print("ğŸ“– Loaded contacts from JSON:")
print(f"Alice's email: {loaded_contacts['Alice']['email']}")
print(f"Bob is active: {loaded_contacts['Bob']['active']}")
print(f"Full data type: {type(loaded_contacts)}")

"""### ğŸ§  In-Class Exercise: Save and Load a Configuration (Page 14)

**Instructions:**
1. Create a Python dictionary: `settings = {"theme": "dark", "notifications_enabled": True, "font_size": 14}`
2. Use the `json` module to dump this into `settings.json`
3. Write code that loads `settings.json` and prints the "theme" value
"""

# Exercise Solution: Configuration Manager
import json

# Step 1: Create settings dictionary
settings = {
    "theme": "dark",
    "notifications_enabled": True,
    "font_size": 14,
    "language": "English"
}

# Step 2: Save to JSON file
with open("settings.json", "w") as f:
    json.dump(settings, f, indent=4)

print("âœ… Settings saved to settings.json")

# Step 3: Load and use settings
with open("settings.json", "r") as f:
    loaded_settings = json.load(f)

print("\nâš™ï¸ Current Settings:")
print("=" * 20)
print(f"Theme: {loaded_settings['theme']}")
print(f"Notifications: {'Enabled' if loaded_settings['notifications_enabled'] else 'Disabled'}")
print(f"Font Size: {loaded_settings['font_size']}")
print(f"Language: {loaded_settings['language']}")

"""## 3. ğŸ† Best Practices & The Hands-On Lab

### ğŸš¨ Error Handling for Files

What happens if you try to read a file that doesn't exist? Your program crashes with a `FileNotFoundError`. We can handle this gracefully using `try...except`.
"""

# Error handling for file operations
def read_file_safely(filename):
    try:
        with open(filename, "r") as f:
            content = f.read()
        return content
    except FileNotFoundError:
        return f"âŒ Error: The file '{filename}' was not found."
    except PermissionError:
        return f"âŒ Error: You don't have permission to read '{filename}'."
    except Exception as e:
        return f"âŒ Unexpected error: {e}"

# Test error handling
print(read_file_safely("non_existent_file.txt"))
print(read_file_safely("settings.json"))  # This should work

"""### ğŸ§ª Hands-On Lab: Log File Analyzer

**Goal:** You are given a log file from a server. Your task is to read the file, parse it, and count the number of "ERROR" and "WARNING" messages.

**Sample log.txt:**

INFO: User login successful

ERROR: Database connection failed

WARNING: Disk space running low

INFO: File uploaded successfully

ERROR: Invalid user input

WARNING: High memory usage

"""

# First, let's create the sample log file
log_content = """INFO: User login successful
ERROR: Database connection failed
WARNING: Disk space running low
INFO: File uploaded successfully
ERROR: Invalid user input
WARNING: High memory usage
INFO: Backup completed
ERROR: Network timeout
WARNING: CPU temperature high"""

with open("log.txt", "w") as f:
    f.write(log_content)

print("Created sample log.txt")

"""### ğŸ”§ Part 1: Reading the File"""

# Part 1: Basic log analyzer
log_counts = {"ERROR": 0, "WARNING": 0, "INFO": 0}

with open("log.txt", "r") as f:
    for line in f:
        line = line.strip()
        if not line:  # Skip empty lines
            continue

        # Split by colon to get log level
        parts = line.split(":", 1)  # Split into 2 parts at most
        if len(parts) >= 2:
            log_level = parts[0].strip()
            message = parts[1].strip()

            # Count the log level
            if log_level in log_counts:
                log_counts[log_level] += 1

print("Basic Log Analysis:")
print("=" * 25)
for level, count in log_counts.items():
    print(f"{level}: {count}")

"""### ğŸ”§ Part 2: Enhanced Parsing and Counting"""

# Part 2: More robust log analyzer
def analyze_log(filepath):
    """Analyze a log file and count message types."""
    log_counts = {"ERROR": 0, "WARNING": 0, "INFO": 0, "OTHER": 0}

    try:
        with open(filepath, "r") as f:
            for line_number, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                # Parse log level (more robust method)
                if ":" in line:
                    log_level = line.split(":", 1)[0].strip()
                else:
                    log_level = "OTHER"

                # Count the log level
                if log_level in log_counts:
                    log_counts[log_level] += 1
                else:
                    log_counts["OTHER"] += 1

        return log_counts

    except FileNotFoundError:
        print(f"âŒ Error: File '{filepath}' not found.")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return None

# Test the enhanced analyzer
results = analyze_log("log.txt")
if results:
    print("\nEnhanced Log Analysis:")
    print("=" * 25)
    for level, count in results.items():
        print(f"{level}: {count}")

"""### ğŸ”§ Part 3: Reporting and Bonus Features"""

# Part 3: Complete solution with all bonus features
import csv
import sys
import json

def advanced_log_analyzer(filepath, output_csv=None, output_json=None):
    """
    Advanced log file analyzer with multiple output options.

    Args:
        filepath (str): Path to the log file
        output_csv (str): Optional path for CSV output
        output_json (str): Optional path for JSON output

    Returns:
        dict: Counts of each log level
    """
    log_counts = {"ERROR": 0, "WARNING": 0, "INFO": 0, "OTHER": 0}

    try:
        with open(filepath, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                # Parse log level
                if ":" in line:
                    log_level = line.split(":", 1)[0].strip().upper()
                else:
                    log_level = "OTHER"

                # Count the log level
                if log_level in log_counts:
                    log_counts[log_level] += 1
                else:
                    log_counts["OTHER"] += 1

        # Generate reports
        print("\n" + "=" * 40)
        print("ğŸ“Š LOG ANALYSIS REPORT")
        print("=" * 40)

        total_messages = sum(log_counts.values())
        for level, count in log_counts.items():
            percentage = (count / total_messages) * 100 if total_messages > 0 else 0
            print(f"{level:8}: {count:3} messages ({percentage:5.1f}%)")

        # CSV Output
        if output_csv:
            with open(output_csv, "w", newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["LogLevel", "Count"])
                for level, count in log_counts.items():
                    writer.writerow([level, count])
            print(f"\nâœ… CSV report saved to: {output_csv}")

        # JSON Output
        if output_json:
            report_data = {
                "filename": filepath,
                "analysis_date": "2025-09-01",
                "counts": log_counts,
                "total_messages": total_messages
            }

            with open(output_json, "w") as f:
                json.dump(report_data, f, indent=4)
            print(f"âœ… JSON report saved to: {output_json}")

        return log_counts

    except FileNotFoundError:
        print(f"âŒ Error: File '{filepath}' not found.")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return None

# Run the advanced analyzer
advanced_log_analyzer("log.txt", output_csv="log_report.csv", output_json="log_report.json")

# Show the generated reports
print("\nGenerated CSV content:")
with open("log_report.csv", "r") as f:
    print(f.read())

print("\nGenerated JSON content:")
with open("log_report.json", "r") as f:
    print(f.read())

"""### ğŸš€ Challenge: Command-Line Interface"""

# Challenge: Command-line version
import sys

def main():
    """Main function for command-line usage."""
    if len(sys.argv) < 2:
        print("Usage: python log_analyzer.py <logfile> [output_csv] [output_json]")
        print("Example: python log_analyzer.py log.txt report.csv report.json")
        sys.exit(1)

    log_file = sys.argv[1]
    output_csv = sys.argv[2] if len(sys.argv) > 2 else None
    output_json = sys.argv[3] if len(sys.argv) > 3 else None

    print(f"Analyzing log file: {log_file}")
    results = advanced_log_analyzer(log_file, output_csv, output_json)

    if results is None:
        sys.exit(1)

# Simulate command-line arguments for demonstration
print("Simulating command-line execution:")
sys.argv = ["log_analyzer.py", "log.txt", "cli_report.csv", "cli_report.json"]
main()

"""## 4. ğŸ“š Comprehensive File I/O Summary

### ğŸ¯ What We Learned:

1.  **File Basics:** Reading and writing text files with `with open()`
2.  **File Modes:** `'r'` (read), `'w'` (write), `'a'` (append)
3.  **Reading Methods:** `.read()`, `.readlines()`, and looping
4.  **Structured Data:** CSV and JSON formats for complex data
5.  **Error Handling:** `try...except` for robust file operations
6.  **Real-world Application:** Log file analyzer

### ğŸ”‘ Key Concepts:

- **Persistence:** Saving data beyond program execution
- **Context Managers:** `with` statements for safe file handling
- **Serialization:** Converting objects to/from storage formats
- **Structured Formats:** CSV for tables, JSON for complex objects

### ğŸ’¡ Real-world Applications:

- **Configuration Files:** Saving user settings
- **Data Export/Import:** CSV for spreadsheet data
- **Web APIs:** JSON for data exchange
- **Logging:** Recording application events
- **Data Processing:** Analyzing large text files

### ğŸš€ Best Practices:

1.  **Always use `with` statements** for automatic file closing
2.  **Handle exceptions** for missing files and permission errors
3.  **Use appropriate formats:** CSV for tables, JSON for objects
4.  **Validate file paths** before operations
5.  **Close files explicitly** if not using `with` statements

### ğŸ“Š File Mode Comparison:

| Mode | Creates File | Overwrites | Position | Use Case |
|:-----|:------------:|:----------:|:--------:|:---------|
| **'r'** | âŒ | âŒ | Start | Reading existing files |
| **'w'** | âœ… | âœ… | Start | Creating new files |
| **'a'** | âœ… | âŒ | End | Adding to existing files |
| **'r+'** | âŒ | âŒ | Start | Reading and writing |
| **'w+'** | âœ… | âœ… | Start | Reading and writing (truncates) |
| **'a+'** | âœ… | âŒ | End | Reading and appending |

### âœ… Completion Checklist:

- [x] Created and read text files with different modes
- [x] Used CSV module for tabular data
- [x] Used JSON module for complex data structures
- [x] Implemented error handling for file operations
- [x] Built a complete log file analyzer
- [x] Added bonus features (CSV/JSON output, CLI interface)

### ğŸ”® Next Steps:

1.  **Explore binary file handling** (images, videos)
2.  **Learn about databases** for more complex data storage
3.  **Study web frameworks** that use JSON for APIs
4.  **Explore data analysis libraries** like pandas for CSV processing
5.  **Learn about logging modules** for professional applications

This foundation in file I/O will enable you to build applications that remember data, interact with external files, and process real-world data formats!
"""